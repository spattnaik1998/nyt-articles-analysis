{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# NYT 500K Articles - Embeddings Generation\n",
    "\n",
    "This notebook:\n",
    "1. Preprocesses 500K NYT articles from CSV\n",
    "2. Generates BERTweet embeddings (768 dimensions)\n",
    "3. Downloads embeddings for local use\n",
    "\n",
    "**Estimated time:** 2-3 hours with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch pandas pyarrow tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from google.colab import files\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload"
   },
   "source": [
    "## Step 3: Upload Your Data\n",
    "\n",
    "Upload `nyt_articles_500K.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_file"
   },
   "outputs": [],
   "source": [
    "# Option 1: Upload file directly\n",
    "print(\"Please upload your nyt_articles_500K.csv file:\")\n",
    "uploaded = files.upload()\n",
    "input_file = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded: {input_file}\")\n",
    "\n",
    "# Option 2: If using Google Drive (uncomment below)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# input_file = '/content/drive/MyDrive/nyt_articles_500K.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## Step 4: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Loaded {len(df):,} articles\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['pub_date'].min()} to {df['pub_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clean_function"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return ''\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?;:\\'-]', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "print(\"Text cleaning function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess_data"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING 500K ARTICLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\n1. Handling missing values...\")\n",
    "df['headline'] = df['headline'].fillna('')\n",
    "df['abstract'] = df['abstract'].fillna('')\n",
    "df['lead_paragraph'] = df['lead_paragraph'].fillna('')\n",
    "\n",
    "# Clean text fields\n",
    "print(\"\\n2. Cleaning text fields...\")\n",
    "tqdm.pandas(desc=\"Cleaning headlines\")\n",
    "df['headline_cleaned'] = df['headline'].progress_apply(clean_text)\n",
    "\n",
    "tqdm.pandas(desc=\"Cleaning abstracts\")\n",
    "df['abstract_cleaned'] = df['abstract'].progress_apply(clean_text)\n",
    "\n",
    "tqdm.pandas(desc=\"Cleaning body\")\n",
    "df['body_cleaned'] = df['lead_paragraph'].progress_apply(clean_text)\n",
    "\n",
    "# Create combined text field\n",
    "print(\"\\n3. Creating combined text field...\")\n",
    "def combine_text(row):\n",
    "    \"\"\"Combine headline, abstract, and body with weights.\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Headline (most important, repeat 2x)\n",
    "    if row.get('headline_cleaned'):\n",
    "        parts.append(row['headline_cleaned'])\n",
    "        parts.append(row['headline_cleaned'])\n",
    "    \n",
    "    # Abstract\n",
    "    if row.get('abstract_cleaned'):\n",
    "        parts.append(row['abstract_cleaned'])\n",
    "    \n",
    "    # Body (first 500 chars)\n",
    "    if row.get('body_cleaned'):\n",
    "        body = str(row['body_cleaned'])[:500]\n",
    "        parts.append(body)\n",
    "    \n",
    "    return ' '.join(parts)\n",
    "\n",
    "tqdm.pandas(desc=\"Combining text\")\n",
    "df['combined_text'] = df.progress_apply(combine_text, axis=1)\n",
    "\n",
    "# Clean combined text\n",
    "print(\"\\n4. Cleaning combined text...\")\n",
    "tqdm.pandas(desc=\"Final cleaning\")\n",
    "df['cleaned_text'] = df['combined_text'].progress_apply(clean_text)\n",
    "\n",
    "# Compute statistics\n",
    "print(\"\\n5. Computing text statistics...\")\n",
    "df['word_count'] = df['cleaned_text'].str.split().str.len()\n",
    "df['char_count'] = df['cleaned_text'].str.len()\n",
    "\n",
    "# Filter out very short articles\n",
    "before_filter = len(df)\n",
    "df = df[df['word_count'] >= 10].reset_index(drop=True)\n",
    "after_filter = len(df)\n",
    "\n",
    "print(f\"\\n6. Filtered out {before_filter - after_filter:,} articles with < 10 words\")\n",
    "print(f\"   Remaining: {after_filter:,} articles\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal dataset: {len(df):,} articles\")\n",
    "print(f\"Avg word count: {df['word_count'].mean():.2f}\")\n",
    "print(f\"Avg char count: {df['char_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "embeddings"
   },
   "source": [
    "## Step 5: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_model"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"✓ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"⚠ Using CPU (this will be slower)\")\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embedding_function"
   },
   "outputs": [],
   "source": [
    "def extract_embeddings_batch(texts, tokenizer, model, device, max_length=128):\n",
    "    \"\"\"Extract embeddings for a batch of texts.\"\"\"\n",
    "    # Tokenize\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "    \n",
    "    # Use CLS token embedding (first token)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "print(\"Embedding extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING BERTWEET MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_name = 'vinai/bertweet-base'\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "embedding_dim = model.config.hidden_size\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_embeddings"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING EMBEDDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration\n",
    "batch_size = 64  # Use 32 if you get OOM errors\n",
    "max_length = 128\n",
    "\n",
    "print(f\"\\nProcessing {len(df):,} articles\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Max length: {max_length}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Estimated size\n",
    "estimated_size_mb = len(df) * embedding_dim * 4 / (1024**2)\n",
    "print(f\"\\nEstimated output size: {estimated_size_mb:.2f} MB\")\n",
    "\n",
    "# Extract embeddings\n",
    "texts = df['cleaned_text'].fillna('').astype(str).tolist()\n",
    "all_embeddings = []\n",
    "n_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"\\nProcessing {n_batches} batches...\\n\")\n",
    "\n",
    "for i in tqdm(range(n_batches), desc=\"Generating embeddings\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(texts))\n",
    "    batch_texts = texts[start_idx:end_idx]\n",
    "    \n",
    "    try:\n",
    "        batch_embeddings = extract_embeddings_batch(\n",
    "            batch_texts,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            device,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠ Error in batch {i}: {e}\")\n",
    "        # Create zero embeddings for failed batch\n",
    "        batch_embeddings = np.zeros((len(batch_texts), embedding_dim))\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "\n",
    "# Combine all embeddings\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "print(f\"\\n✓ Embeddings generated successfully\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Size: {embeddings.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_mapping"
   },
   "outputs": [],
   "source": [
    "print(\"\\nCreating ID-to-index mapping...\")\n",
    "\n",
    "# Create mapping DataFrame\n",
    "if '_id' in df.columns:\n",
    "    mapping = pd.DataFrame({\n",
    "        '_id': df['_id'].values,\n",
    "        'index': np.arange(len(df))\n",
    "    })\n",
    "else:\n",
    "    mapping = pd.DataFrame({\n",
    "        '_id': df.index.values,\n",
    "        'index': np.arange(len(df))\n",
    "    })\n",
    "\n",
    "print(f\"✓ Mapping created: {len(mapping):,} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## Step 6: Save and Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_files"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save embeddings\n",
    "print(\"\\n1. Saving embeddings...\")\n",
    "np.save('embeddings_500k.npy', embeddings)\n",
    "print(f\"   ✓ Saved: embeddings_500k.npy ({embeddings.nbytes / (1024**2):.2f} MB)\")\n",
    "\n",
    "# Save mapping\n",
    "print(\"\\n2. Saving mapping...\")\n",
    "mapping.to_csv('embeddings_500k_mapping.csv', index=False)\n",
    "print(f\"   ✓ Saved: embeddings_500k_mapping.csv ({len(mapping):,} rows)\")\n",
    "\n",
    "# Save preprocessed data\n",
    "print(\"\\n3. Saving preprocessed data...\")\n",
    "df.to_parquet('preprocessed_500K.parquet', index=False)\n",
    "print(f\"   ✓ Saved: preprocessed_500K.parquet ({len(df):,} articles)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL FILES SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "print(\"\\nDownloading files to your computer...\\n\")\n",
    "\n",
    "# Download embeddings\n",
    "print(\"1. Downloading embeddings_500k.npy...\")\n",
    "files.download('embeddings_500k.npy')\n",
    "\n",
    "# Download mapping\n",
    "print(\"2. Downloading embeddings_500k_mapping.csv...\")\n",
    "files.download('embeddings_500k_mapping.csv')\n",
    "\n",
    "# Download preprocessed data\n",
    "print(\"3. Downloading preprocessed_500K.parquet...\")\n",
    "files.download('preprocessed_500K.parquet')\n",
    "\n",
    "print(\"\\n✓ All files downloaded!\")\n",
    "print(\"\\nPlace these files in your local 'data/' directory:\")\n",
    "print(\"  - embeddings_500k.npy\")\n",
    "print(\"  - embeddings_500k_mapping.csv\")\n",
    "print(\"  - preprocessed_500K.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verification"
   },
   "source": [
    "## Step 7: Verification & Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Mapping shape: {mapping.shape}\")\n",
    "print(f\"\\nFirst 5 article IDs:\")\n",
    "print(mapping.head())\n",
    "print(f\"\\nSample embedding (first 10 dimensions):\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_similarity"
   },
   "outputs": [],
   "source": [
    "# Quick similarity test\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK SIMILARITY TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find articles similar to the first article\n",
    "query_idx = 0\n",
    "similarities = cosine_similarity([embeddings[query_idx]], embeddings)[0]\n",
    "top_k_indices = similarities.argsort()[-10:][::-1]\n",
    "\n",
    "print(f\"\\nTop 10 articles similar to article at index {query_idx}:\")\n",
    "print(f\"Query article ID: {mapping.iloc[query_idx]['_id']}\")\n",
    "print(f\"\\nSimilar articles:\")\n",
    "for i, idx in enumerate(top_k_indices, 1):\n",
    "    article_id = mapping.iloc[idx]['_id']\n",
    "    score = similarities[idx]\n",
    "    print(f\"  {i}. {article_id} (similarity: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What You Generated:\n",
    "1. **embeddings_500k.npy** - BERTweet embeddings (500K × 768)\n",
    "2. **embeddings_500k_mapping.csv** - Article ID to index mapping\n",
    "3. **preprocessed_500K.parquet** - Cleaned article data\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the three files above\n",
    "2. Place them in your local `data/` directory\n",
    "3. Update your application to use the new embeddings\n",
    "4. Run your analysis with 25x more data!\n",
    "\n",
    "### Local Usage:\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load embeddings\n",
    "embeddings = np.load('data/embeddings_500k.npy')\n",
    "mapping = pd.read_csv('data/embeddings_500k_mapping.csv')\n",
    "articles = pd.read_parquet('data/preprocessed_500K.parquet')\n",
    "\n",
    "print(f\"Loaded {embeddings.shape[0]:,} embeddings\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
