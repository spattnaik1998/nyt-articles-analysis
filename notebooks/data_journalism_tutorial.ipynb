{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYT Data Journalism Tutorial\n",
        "\n",
        "This tutorial demonstrates how to perform comprehensive text analysis on New York Times articles using modern NLP techniques. We'll cover:\n",
        "\n",
        "1. **Data Loading & Preprocessing** - Loading and preparing NYT article data\n",
        "2. **Topic Modeling** - Discovering themes using LDA and BERTopic\n",
        "3. **Sentiment Analysis** - Analyzing sentiment with FinBERT and PoliBERT\n",
        "4. **Embeddings & Similarity Search** - Creating and using text embeddings\n",
        "5. **Book Extraction** - Extracting structured data with LLMs\n",
        "6. **Visualizations** - Creating insightful visual representations\n",
        "\n",
        "Each section demonstrates features that are available in the modular implementation located in the `src/` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading & Preprocessing\n",
        "\n",
        "First, we'll load the NYT dataset and prepare it for analysis. The modular implementation provides this functionality in `src/ingest/load_nyt.py` and `src/preprocess/text.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# For this tutorial, we'll assume the data is already downloaded\n",
        "# In production, use: from src.ingest.load_nyt import load_nyt_data\n",
        "\n",
        "# Load the dataset (adjust path as needed)\n",
        "# df = pd.read_csv('path_to_nyt_data.csv')\n",
        "\n",
        "# For demonstration, we'll show the expected structure\n",
        "print(\"Expected columns: _id, pub_date, headline, abstract, lead_paragraph, section_name, document_type, web_url\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert pub_date to datetime and create working dataframe\n",
        "df['pub_date'] = pd.to_datetime(df['pub_date'], errors='coerce')\n",
        "df.dropna(subset=['pub_date'], inplace=True)\n",
        "\n",
        "# Create a clean working copy with essential columns\n",
        "articles_df = df[[\n",
        "    '_id', 'pub_date', 'headline', 'web_url', 'abstract', \n",
        "    'lead_paragraph', 'section_name', 'document_type'\n",
        "]].copy()\n",
        "\n",
        "print(f\"Total articles: {len(articles_df):,}\")\n",
        "print(f\"\\nTop sections:\")\n",
        "print(articles_df['section_name'].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "The `src/preprocess/text.py` module provides utilities for cleaning and combining text fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if needed\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "def combine_text_fields(df, columns=['headline', 'abstract', 'lead_paragraph']):\n",
        "    \"\"\"Combine multiple text columns into one.\"\"\"\n",
        "    combined = df[columns[0]].astype(str)\n",
        "    for col in columns[1:]:\n",
        "        combined = combined + ' ' + df[col].astype(str)\n",
        "    return combined.str.replace('nan', '').str.strip()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing punctuation, numbers, and stopwords.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if w not in stop_words and len(w) > 2]\n",
        "    return ' '.join(words)\n",
        "\n",
        "print(\"Preprocessing functions ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Topic Modeling\n",
        "\n",
        "Topic modeling helps discover hidden themes in large text collections. We'll demonstrate both LDA (Latent Dirichlet Allocation) and BERTopic, both available in `src/models/topic_models.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 LDA Topic Modeling\n",
        "\n",
        "Let's analyze World section articles from 2001 to discover major themes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter World articles from 2001\n",
        "world_2001 = articles_df[\n",
        "    (articles_df['pub_date'].dt.year == 2001) &\n",
        "    (articles_df['section_name'] == 'World')\n",
        "].copy()\n",
        "\n",
        "# Combine and clean text\n",
        "world_2001['combined_text'] = combine_text_fields(world_2001)\n",
        "world_2001['cleaned_text'] = world_2001['combined_text'].apply(clean_text)\n",
        "\n",
        "print(f\"World articles in 2001: {len(world_2001):,}\")\n",
        "print(f\"Sample: {world_2001['combined_text'].iloc[0][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install gensim for LDA\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "\n",
        "# Tokenize\n",
        "world_2001['tokens'] = world_2001['combined_text'].apply(\n",
        "    lambda x: simple_preprocess(x, deacc=True)\n",
        ")\n",
        "\n",
        "# Create dictionary and corpus\n",
        "dictionary = Dictionary(world_2001['tokens'])\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in world_2001['tokens']]\n",
        "\n",
        "print(f\"Dictionary size: {len(dictionary)} unique tokens\")\n",
        "print(f\"Corpus size: {len(corpus)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train LDA model\n",
        "num_topics = 10\n",
        "lda_model = LdaMulticore(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=10,\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "# Display topics\n",
        "print(\"\\nTop 10 Topics from LDA Model:\\n\")\n",
        "for idx, topic in lda_model.print_topics(num_words=8):\n",
        "    print(f\"Topic {idx}: {topic}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 BERTopic Modeling\n",
        "\n",
        "BERTopic uses transformer embeddings for more semantic topic discovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install BERTopic and dependencies\n",
        "!{sys.executable} -m pip install -q bertopic umap-learn hdbscan sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "# Prepare documents\n",
        "documents = world_2001['combined_text'].tolist()\n",
        "\n",
        "# Train BERTopic model\n",
        "topic_model = BERTopic(\n",
        "    language='english',\n",
        "    calculate_probabilities=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(documents)\n",
        "\n",
        "# Display topic info\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(\"\\nBERTopic Results:\")\n",
        "print(topic_info.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize topics\n",
        "fig = topic_model.visualize_topics(width=1200, height=800)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sentiment Analysis\n",
        "\n",
        "We'll use domain-specific sentiment models: PoliBERT for opinion pieces and FinBERT for business articles. These are implemented in `src/models/sentiment.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 PoliBERT for Opinion Articles\n",
        "\n",
        "PoliBERT (RoBERTa-based) is designed for political sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install transformers\n",
        "!{sys.executable} -m pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter Opinion articles from 2001\n",
        "opinion_2001 = articles_df[\n",
        "    (articles_df['pub_date'].dt.year == 2001) &\n",
        "    (articles_df['section_name'] == 'Opinion')\n",
        "].copy()\n",
        "\n",
        "opinion_2001['combined_text'] = combine_text_fields(opinion_2001)\n",
        "\n",
        "print(f\"Opinion articles in 2001: {len(opinion_2001):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load PoliBERT (using Twitter-RoBERTa as proxy)\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "print(\"PoliBERT model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_sentiment_polibert(text):\n",
        "    \"\"\"Analyze sentiment using PoliBERT.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 'Neutral', {'negative': 0.0, 'neutral': 1.0, 'positive': 0.0}\n",
        "    \n",
        "    # Tokenize and get predictions\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Get probabilities\n",
        "    probs = F.softmax(outputs.logits, dim=1)[0]\n",
        "    scores = {'negative': probs[0].item(), 'neutral': probs[1].item(), 'positive': probs[2].item()}\n",
        "    label = max(scores, key=scores.get).capitalize()\n",
        "    \n",
        "    return label, scores\n",
        "\n",
        "# Test on a sample\n",
        "sample_text = opinion_2001['combined_text'].iloc[0]\n",
        "label, scores = analyze_sentiment_polibert(sample_text)\n",
        "print(f\"Sample sentiment: {label}\")\n",
        "print(f\"Scores: {scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Analyze a sample (first 100 articles for speed)\n",
        "sample = opinion_2001.head(100).copy()\n",
        "results = sample['combined_text'].apply(analyze_sentiment_polibert)\n",
        "sample['sentiment_label'] = results.apply(lambda x: x[0])\n",
        "sample['sentiment_scores'] = results.apply(lambda x: x[1])\n",
        "\n",
        "print(\"Sentiment distribution:\")\n",
        "print(sample['sentiment_label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=sample['sentiment_label'].value_counts().index, \n",
        "            y=sample['sentiment_label'].value_counts().values)\n",
        "plt.title('Opinion Article Sentiment Distribution (PoliBERT)')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 FinBERT for Business Articles\n",
        "\n",
        "FinBERT is specialized for financial sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter Business Day articles from 2001\n",
        "business_2001 = articles_df[\n",
        "    (articles_df['pub_date'].dt.year == 2001) &\n",
        "    (articles_df['section_name'] == 'Business Day')\n",
        "].copy()\n",
        "\n",
        "business_2001['combined_text'] = combine_text_fields(business_2001)\n",
        "\n",
        "print(f\"Business Day articles in 2001: {len(business_2001):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load FinBERT\n",
        "finbert_model_name = \"ProsusAI/finbert\"\n",
        "finbert_tokenizer = AutoTokenizer.from_pretrained(finbert_model_name)\n",
        "finbert_model = AutoModelForSequenceClassification.from_pretrained(finbert_model_name)\n",
        "\n",
        "print(\"FinBERT model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_sentiment_finbert(text):\n",
        "    \"\"\"Analyze financial sentiment using FinBERT.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 'Neutral', {'negative': 0.0, 'neutral': 1.0, 'positive': 0.0}\n",
        "    \n",
        "    # Tokenize and get predictions\n",
        "    inputs = finbert_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = finbert_model(**inputs)\n",
        "    \n",
        "    # Get probabilities (FinBERT: positive=0, negative=1, neutral=2)\n",
        "    probs = F.softmax(outputs.logits, dim=1)[0]\n",
        "    scores = {'positive': probs[0].item(), 'negative': probs[1].item(), 'neutral': probs[2].item()}\n",
        "    label = max(scores, key=scores.get).capitalize()\n",
        "    \n",
        "    return label, scores\n",
        "\n",
        "# Test on a sample\n",
        "sample_text = business_2001['combined_text'].iloc[0]\n",
        "label, scores = analyze_sentiment_finbert(sample_text)\n",
        "print(f\"Sample sentiment: {label}\")\n",
        "print(f\"Scores: {scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze a sample (first 100 articles for speed)\n",
        "business_sample = business_2001.head(100).copy()\n",
        "results = business_sample['combined_text'].apply(analyze_sentiment_finbert)\n",
        "business_sample['finbert_sentiment'] = results.apply(lambda x: x[0])\n",
        "business_sample['finbert_scores'] = results.apply(lambda x: x[1])\n",
        "\n",
        "print(\"FinBERT sentiment distribution:\")\n",
        "print(business_sample['finbert_sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize FinBERT sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=business_sample['finbert_sentiment'].value_counts().index, \n",
        "            y=business_sample['finbert_sentiment'].value_counts().values)\n",
        "plt.title('Business Article Sentiment Distribution (FinBERT)')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Embeddings & Similarity Search\n",
        "\n",
        "Text embeddings enable semantic similarity search. This functionality is in `src/models/embeddings.py` and `src/models/similarity.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Load embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"Embedding model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embeddings for a sample of articles\n",
        "sample_articles = world_2001.head(50).copy()\n",
        "texts = sample_articles['combined_text'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "print(f\"Created {len(embeddings)} embeddings of dimension {embeddings.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find similar articles to a query\n",
        "query = \"terrorism and security measures\"\n",
        "query_embedding = embedding_model.encode([query])\n",
        "\n",
        "# Calculate similarities\n",
        "similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
        "\n",
        "# Get top 5 most similar articles\n",
        "top_indices = np.argsort(similarities)[-5:][::-1]\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Top 5 most similar articles:\\n\")\n",
        "for i, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. Similarity: {similarities[idx]:.3f}\")\n",
        "    print(f\"   {sample_articles.iloc[idx]['combined_text'][:150]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Book Extraction\n",
        "\n",
        "Extract structured book and author information using LLMs. This is implemented in `src/models/extraction.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!{sys.executable} -m pip install -q pydantic openai instructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter Books section articles\n",
        "books_2001 = articles_df[\n",
        "    (articles_df['pub_date'].dt.year == 2001) &\n",
        "    (articles_df['section_name'] == 'Books')\n",
        "].copy()\n",
        "\n",
        "books_2001['combined_text'] = combine_text_fields(books_2001)\n",
        "\n",
        "print(f\"Books articles in 2001: {len(books_2001):,}\")\n",
        "print(f\"\\nSample: {books_2001['combined_text'].iloc[0][:300]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "import json\n",
        "\n",
        "# Define schema for book extraction\n",
        "class BookAuthor(BaseModel):\n",
        "    book_title: str\n",
        "    author_name: str\n",
        "\n",
        "print(\"Pydantic schema defined:\")\n",
        "print(json.dumps(BookAuthor.model_json_schema(), indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: This requires an OpenAI API key\n",
        "# For demonstration purposes, we'll show the structure without actual API calls\n",
        "\n",
        "print(\"\"\"\n",
        "Book Extraction Process:\n",
        "\n",
        "1. The text is sent to an LLM (e.g., GPT-4) with structured output requirements\n",
        "2. The LLM extracts book titles and author names\n",
        "3. Results are validated against the Pydantic schema\n",
        "4. Structured data is stored for further analysis\n",
        "\n",
        "Example usage:\n",
        "```python\n",
        "import openai\n",
        "import instructor\n",
        "\n",
        "client = instructor.from_openai(openai.OpenAI(api_key=\"your-key\"))\n",
        "\n",
        "def extract_book_info(text):\n",
        "    return client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        response_model=BookAuthor,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Extract book title and author from: {text}\"\n",
        "        }]\n",
        "    )\n",
        "```\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations\n",
        "\n",
        "Create visual representations of the data insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install wordcloud\n",
        "!{sys.executable} -m pip install -q wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create word cloud for World articles\n",
        "text_data = ' '.join(world_2001['cleaned_text'].dropna())\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    width=1200,\n",
        "    height=600,\n",
        "    background_color='white',\n",
        "    colormap='viridis',\n",
        "    max_words=100\n",
        ").generate(text_data)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud: World Articles 2001', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create section distribution pie chart\n",
        "section_counts = articles_df[\n",
        "    articles_df['pub_date'].dt.year == 2001\n",
        "]['section_name'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.pie(section_counts, labels=section_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Top 10 Sections Distribution (2001)', fontsize=14, fontweight='bold')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series of article counts\n",
        "monthly_counts = articles_df[\n",
        "    articles_df['pub_date'].dt.year == 2001\n",
        "].groupby(articles_df['pub_date'].dt.month).size()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_counts.index, monthly_counts.values, marker='o', linewidth=2, markersize=8)\n",
        "plt.xlabel('Month', fontsize=12)\n",
        "plt.ylabel('Number of Articles', fontsize=12)\n",
        "plt.title('NYT Articles Published per Month in 2001', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This tutorial demonstrated the key capabilities of the NYT Data Journalism analysis system:\n",
        "\n",
        "1. **Data Loading & Preprocessing** - Efficiently loading and cleaning large-scale article data\n",
        "2. **Topic Modeling** - Discovering themes with both classical (LDA) and modern (BERTopic) approaches\n",
        "3. **Sentiment Analysis** - Domain-specific sentiment analysis with FinBERT and PoliBERT\n",
        "4. **Embeddings & Similarity** - Semantic search and article similarity using transformer embeddings\n",
        "5. **Book Extraction** - Structured data extraction using LLM-based parsing\n",
        "6. **Visualizations** - Creating insightful visualizations of text data\n",
        "\n",
        "### Modular Implementation\n",
        "\n",
        "All these features are available in the production-ready modular implementation:\n",
        "\n",
        "```\n",
        "src/\n",
        "\u251c\u2500\u2500 ingest/\n",
        "\u2502   \u2514\u2500\u2500 load_nyt.py          # Data loading utilities\n",
        "\u251c\u2500\u2500 preprocess/\n",
        "\u2502   \u2514\u2500\u2500 text.py              # Text preprocessing functions\n",
        "\u251c\u2500\u2500 models/\n",
        "\u2502   \u251c\u2500\u2500 topic_models.py      # LDA and BERTopic implementations\n",
        "\u2502   \u251c\u2500\u2500 sentiment.py         # Multi-model sentiment analysis\n",
        "\u2502   \u251c\u2500\u2500 embeddings.py        # Text embedding generation\n",
        "\u2502   \u251c\u2500\u2500 similarity.py        # Similarity search functionality\n",
        "\u2502   \u2514\u2500\u2500 extraction.py        # LLM-based extraction\n",
        "\u2514\u2500\u2500 api/\n",
        "    \u251c\u2500\u2500 app.py               # Flask API application\n",
        "    \u2514\u2500\u2500 main.py              # API entry point\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the API endpoints in `src/api/app.py` for integrating these features into applications\n",
        "- Scale analysis to multiple years and sections\n",
        "- Experiment with different topic counts and model parameters\n",
        "- Build custom visualizations for specific use cases\n",
        "- Extend the extraction module for other entity types (people, organizations, etc.)\n",
        "\n",
        "For questions or contributions, refer to the project README."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
